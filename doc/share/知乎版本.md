从 Civitai 到端侧推理：Stable Diffusion 1.5 模型转换与部署全流程（附源码）

最近在折腾爱芯派（AX8850）的端侧部署，成功跑通了 Stable Diffusion 1.5 的全流程。在这个过程中踩了一些坑，也总结了一套比较顺手的 workflow，今天把从模型下载、ONNX 导出到最后转换成 `.axmodel` 的全过程分享给大家。

项目相关的所有代码我已经开源，大家可以直接 Clone 下来跟着做：  
👉 **GitHub:** `https://github.com/yuyun2000/sd15-to-ax8850-deploy`

---

### 第一步：模型准备 (Civitai)

部署的第一步当然是选一个心仪的模型。我们需要去 C站 (Civitai) 挑选一个 **SD 1.5** 的底模。

**选模标准（敲黑板）：**

1. **Base Model:** 必须是 SD 1.5。
2. **类型:** 完整的 Checkpoint（大模型）。_注：LoRA 理论上支持，但这篇先讲 Checkpoint，LoRA 留给下一篇教程。_
3. **大小:** 最好是标准的 **1.99GB** 版本（Pruned 版），大一点的 fp32 版本也能用，但建议用标准的。

以 demo 中的 `xxmix9realistic` 为例：  
🔗 模型链接：`https://civitai.com/models/47274/xxmix9realistic`

下载完成后，你会得到一个 `.safetensors` 文件。

---

### 第二步：PC 端环境验证与导出

在开始转换之前，我们需要在 PC 上先把环境搭好，确认模型能跑，并将其导出为 ONNX。

**1. 验证 Safetensors 推理**  
安装好依赖后，先用 Python 脚本跑一下刚才下载的权重，确认模型本身没问题：

bash

折叠

保存复制

1

2

3

4

5

python safetensor_infer.py \

--input_path "./s/xxmix9realistic_v40.safetensors" \

--output_path "./test_output" \

--negative_prompt "easynegative,ng_deepnegative_v1_75t,(worst quality:2),(low quality:2),(normal quality:2),lowres,bad anatomy,bad hands,normal quality,((monochrome)),((grayscale)),((watermark))," \

--prompt "1girl, upper body, (huge Laughing),sweety,sun glare, bokeh, depth of field, blurry background, light particles, strong wind,head tilt,simple background, red background,<lora:film:0.4>"

如果能生成正常的图片，说明底模加载没问题。

**2. 导出 ONNX 模型**  
这是关键的一步，我们需要把 PyTorch 模型转为 ONNX。

bash

折叠

保存复制

1

python export_onnx.py --input_path ./s/xxmix9realistic_v40.safetensors --output_path ./out_onnx --isize 480x320 --prompt "你的提示词..."

**⚠️ 避坑指南（分辨率设置）：**

- `--isize` 参数：`320x480` 是竖图，`480x320` 是横图。
- **关于 512x512：** 虽然可以写 `512x512`，但在后续步骤或板端推理时可能会因为内存或算力限制报错。
- 这里的尺寸必须与后面 VAE 等环节的尺寸对应（存在 8 倍缩放关系）。
- **关于 Text Encoder：** 细心的朋友会发现这里**没有导出 Text Encoder**。原因有两个：一是官方 Demo 里的导出环境有点问题；二是 Text Encoder 基本都用的是标准的 CLIP，完全可以直接复用现成的，没必要重复造轮子。

_(可选) ONNX 推理验证：_  
如果你不放心导出的 ONNX 是否完好，可以运行 `python dpm_infer.py` 验证一下，这一步非必须。

---

### 第三步：准备量化校准集

AXERA 的工具链需要校准集来进行量化。运行以下脚本：

bash

折叠

保存复制

1

python prepare_data.py

脚本会在目录下生成 `calib_data_vae` 和 `calib_data_unet` 两个文件夹。  
_Tip: 如果你换了模型风格，建议在代码里自行修改生成校准集的 Prompt，以保证量化精度。_

---

### 第四步：模型转换 (Pulsar2)

**注意：** 以下操作需要切换到 **Pulsar docker 环境**（目前使用的是 Pulsar 4.2 版本）。

**1. 转换 UNet (重头戏)**  
这是最耗时的一步，建议挂机去喝杯咖啡。

bash

折叠

保存复制

1

pulsar2 build --input out_onnx/unet_sim_cut.onnx --config config_unet_u16.json --output_dir output_unet_ax --output_name unet.axmodel

⏱ **耗时参考：** 在我的环境里，UNet 转换耗时约 **68分钟**。

**2. 转换 VAE**

bash

折叠

保存复制

1

pulsar2 build --input out_onnx/vae_decoder_sim.onnx --config config_vae_decoder_u16.json --output_dir output_vae_ax --output_name vae.axmodel

---

### 第五步：板端部署

到这里，我们已经有了 `unet.axmodel` 和 `vae.axmodel`。还缺一个 Text Encoder。

还记得刚才说 Text Encoder 不需要导出吗？我们可以直接去 HuggingFace 下载已经转换好的通用版本：  
🔗 `https://huggingface.co/yunyu1258/SD1.5-AX650-Dark_Sushi_Mix`

将三个模型文件（UNet, VAE, Text Encoder）拷贝到板子上。GitHub 仓库里也提供了在板子上加载 axmodel 进行推理的 Python 脚本，直接运行即可体验端侧生图！

---

**总结**  
整个流程的核心在于 ONNX 的正确导出和 Pulsar 的编译配置。虽然 UNet 编译时间较长，但看到图片在板子上生成的那一刻，成就感还是满满的。

觉得有帮助的朋友欢迎点赞收藏，有任何报错问题可以在评论区交流！